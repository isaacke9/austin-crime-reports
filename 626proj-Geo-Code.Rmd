---
title: "STAT 626 Project - Crime Reports in Austin"
author: "Geo Lee"
date: "7/4/22"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Data is from [linked phrase](https://data.austintexas.gov/Public-Safety/Crime-Reports/fdj4-gpfu)

# 10 Minute Presentation & 5-Page Report
6. Transform the Data to Stationarity using regression (follow Example 3.5 in the text
as much as possible); Explain differencing, log-transform,.... if used.

7. ACF and PACF Plots: Use correlogram and partial correlogram to formulate ARMA(p,
q) models for the "stationary" data. If in doubt, choose from AR models, these are
simple to estimate, interpret and predict.

8. Fit and Forecast: Estimate the model parameters using simple-minded methods like
the least squares, Yule-Walker estimates, etc.

9. Diagnostic: Check the residuals to see if they are white noise.

10. You may want to consult the first three chapters of the text for notation, terminologies
and ideas.

```{r}
library(data.table)
library(lubridate)
library(ggplot2)
library(dplyr)
library(data.table)
#library(ggrepel)
#library(tidyverse)
#library(RColorBrewer)
library(astsa)
require(forecast)
require(prophet)
# library(qmap)
# library(ggmap)
```

# Load Cleaned Dataset
```{r}
setwd("~/Desktop/Summer 2022/STAT 626")
load("crime.RData")
```

# Data Wrangling (Dates) 
```{r}
# Proper formatting
colnames(crime) = make.names(colnames(crime))
crime = as.data.frame(crime)

# sample size & number features
n = dim(crime)[1]
p = dim(crime)[2]

# Nicely formatted dates, extract portions of dates as new columns
datetime = mdy_hms(crime$Report.Date.Time, tz = "UTC")
crime$ymd = datetime
crime$year = year(crime$ymd)
crime$month = month(crime$ymd)
crime$datetime = datetime
crime$date = as.Date(crime$datetime) 
crime$month_year = as.character(paste(as.numeric(year(crime$ymd)), as.numeric(month(crime$ymd)), as.numeric("1") ,sep="-")) # important
```


```{r}
# # Subsample crime by individual years (easier computation, for basic implementation)
# crime2021 = filter(crime, year == 2021)
# crime2022 = filter(crime, year == 2022)

crimepermonth = crime %>%
  group_by(month_year) %>%
  summarize(Freq=n()) %>%
  filter(row_number()!=1 & row_number()!=n() & row_number()!=n()-1) #%>% # get rid of the endpoint days with 0 
  #filter(row_number()>=1 & row_number()<174) # get rid of outlier dip (from 2-2003 to 3-2022)
crimepermonth$month_year = as.Date(crimepermonth$month_year)
crimepermonth = arrange(crimepermonth,month_year)

# crimeperday = crime %>% # from (1/1/2003 to 4/1/2022)
#   group_by(date) %>%
#   summarize(Freq=n()) %>%
#   filter(row_number()!=1 & row_number()!=n() & row_number()!=n()-1) # get rid of the endpoint days with 0 
# 
# crimeperdaytheft = crime %>%
#   filter(Category.Description=="Theft") %>%
#   group_by(date) %>%
#   summarize(Freq=n()) %>%
#   filter(row_number()!=1 & row_number()!=n() & row_number()!=n()-1)
# 
# crimeperyear = crime %>%
#   group_by(year) %>%
#   summarize(Freq=n()) %>%
#   filter(row_number()!=1 & row_number()!=n() & row_number()!=n()-1)
# 
# crimeperday2021 = crime2021 %>%
#   group_by(date) %>%
#   summarize(Freq=n()) %>%
#   filter(row_number()!=1 & row_number()!=n() & row_number()!=n()-1)

############################
# Crime count per **** 
x = ts(crimepermonth)  # CHANGE
############################

#ma = stats::filter(x[,2], sides = 2, filter = rep(1/12,12))

tsplot(x[,2], main="Monthly Police Reports in Austin", col=4, 
       ylab = "Number of Police Reports", 
       xlab = "Time (number of months since January 2003)")
#lines(ma, type="l", lty=2, col = "red")
#legend(5000, 525, legend=c("Data", "Moving Average"),
#       col=c("blue", "red"), lty=1:1, cex=0.8)

```


## See crime per month (cycle within year)
```{r}
crimemonth = crime %>%
  group_by(month) %>%
  summarize(Freq=n()) %>%
  filter(row_number()!=n()) # get rid of the endpoint days with 0 

crimemonth$month = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
crimemonth$month <- factor(crimemonth$month, levels = crimemonth$month)
crimemonth = as.data.frame(crimemonth)

ggplot(crimemonth, aes(x=month, y=Freq)) + geom_line(aes(group=1)) + geom_point(color="blue") +
    labs(title="Total Monthly Police Reports in Austin", 
         y = "Total Crime Count",
         x= "Month") +
    theme(plot.title=element_text(hjust=0.5))
```


# Kernel Smoother to Estimate Trend
```{r}
tsplot(x[,2], col="blue",
       main = "Monthly Crime Reports in Austin (Kernel Bandwidth=5)", ylab = "Number of Police Reports", 
       xlab = "Time (number of months since January 2003)")  
lines(ksmooth(time(x[,2]), x[,2], "normal", bandwidth=5), lwd=2, col="red")  
legend(130, 13000, legend=c("Original Data", "Normal Kernel Smoother (B=5)"),
       col=c("blue", "red"), lty=1:1, cex=0.8)

tsplot(x[,2], col="blue",
       main = "Monthly Crime Reports in Austin (Kernel Bandwidth=30)", ylab = "Number of Police Reports", 
       xlab="Time (number of months since January 2003)")  
lines(ksmooth(time(x[,2]), x[,2], "normal", bandwidth=30), lwd=2, col="red")  
legend(130, 13000, legend=c("Original Data", "Normal Kernel Smoother (B=30)"),
       col=c("blue", "red"), lty=1:1, cex=0.8)
```


# Check Stationarity 
## mean and variance stationarity check 
```{r}
# Check for equal variance
n = length(x[,2])
var1 = sd(x[,2][1:n/2])^2
var2 = sd(x[,2][((n/2)+1):n])^2
cat("sample variance first half of TS:", var1, "\n")
cat("sample variance second half of TS:", var2, "\n")
cat("ratio of var in 1st to 2nd half:" ,var1/var2*100, "%\n")

# check normality
qqnorm(x[,2], main="Normal QQPlot - Original Data", col=4)
qqline(x[,2], col=2, lwd=2)  

# Check ACF for trend (DON'T ACTUALLY USE THO)
#max.lag=200 # 7000, 220
#acf1(x[,2], main = "Sample ACF - Original TS", max.lag=max.lag)
#acf1(x[,2], main = "Sample PACF - Original TS", max.lag=max.lag, pacf=TRUE)
```


# Transform to Stationary
# First Order Difference 
```{r}
x_diff = diff(x[,2])
tsplot(x_diff, col=4,
       main = "Monthly Police Reports in Austin (First Order Difference)", 
       ylab = "Number of Police Reports (1st Order Difference)",
       xlab = "Time (number of months since January 2003)")

# Check for equal variance
n = length(x_diff)
var1_diff = sd(x_diff[1:n/2])^2
var2_diff = sd(x_diff[((n/2)+1):n])^2
cat("sample variance first half of TS:",var1_diff , "\n")
cat("sample variance second half of TS:", var2_diff, "\n")
cat("ratio of var in 1st to 2nd half:" ,var1_diff/var2_diff*100, "%\n")

# check normality
qqnorm(x_diff, main="Normal QQPlot for Differenced Data", col=4)
qqline(x_diff, col=2, lwd=2)  

max.lag=100 #7000, 220
acf1(x_diff, main = expression("Sample ACF - Differenced Data"), max.lag=max.lag)
acf1(x_diff, main = expression("Sample PACF - Differenced Data"), max.lag=max.lag, pacf=TRUE)
```

# Check for Cyclic Pattern in Differenced Data
```{r}
tsplot(x_diff, col="blue",
       main = "Differenced Monthly Crime Reports in Austin (Kernel Bandwidth=5)", 
       ylab = "Number of Police Reports (1st Order Difference)", 
       xlab = "Time (number of months since January 2003)")  
lines(ksmooth(time(x_diff), x_diff, "normal", bandwidth=5), lwd=2, col="red")  
legend(110, 2300, legend=c("Original Data", "Normal Kernel Smoother (B=5)"),
       col=c("blue", "red"), lty=1:1, cex=0.8) 
```

# ARMA Model Estimation
> choosing q and p in ARMA: https://towardsdatascience.com/identifying-ar-and-ma-terms-using-acf-and-pacf-plots-in-time-series-forecasting-ccb9fd073db8

The ACF and PACF plots should be considered together to define the process. For the AR process, we expect that the ACF plot will gradually decrease and simultaneously the PACF should have a sharp drop after p significant lags. To define a MA process, we expect the opposite from the ACF and PACF plots, meaning that: the ACF should show a sharp drop after a certain q number of lags while PACF should show a geometric or gradual decreasing trend. On the other hand, if both ACF and PACF plots demonstrate a gradual decreasing pattern, then the ARMA process should be considered for modeling.

```{r}
q=8 # AR order
d=0 # degree of differencing (0 for AMRA models - we already took care of differncing)
p=0 # MA order
arma_fit = arima(x_diff, order=c(q,d,p))
summary(arma_fit)
plot(arma_fit$residuals, main = "AR(8) Model Residuals", ylab = "Residuals")
checkresiduals(arma_fit)
```


## Check roots for parameter redundancy
```{r}
(roots = polyroot(c(1,head(-unname(arma_fit$coef), -1))))
Mod(roots)
#base::duplicated(Mod(roots))
```

# Forecast for the next 12 months 
```{r}
autoplot(forecast(arma_fit, level=95, h=12), ylab="Differenced Monthly Police Reports in Austin",
         xlab = "Time (number of months since February 2003)",
         main = "12 Month Forecast from AR(8) Model")
```

## Fit AR(p) with yule-walker estimation (page 83)
```{r}
(ar_yw = ar.yw(x_diff, order = 8))
ar_yw$x.mean # mean estimate
sqrt(diag(ar_yw$asy.var.coef)) # coef standard errors
plot(ar_yw$resid, main = "AR(3) Model Residuals", ylab = "Residuals")
checkresiduals(ar_yw)
```


## Check roots for parameter redundancy
```{r}
(roots2 = polyroot(c(1,-unname(ar_yw$ar))))
Mod(roots2)
#base::duplicated(Mod(roots2))
```

## Forecast
```{r}
autoplot(forecast(ar_yw, level=95, h=12), ylab="Differenced Monthly Police Reports in Austin",
         xlab = "Time (number of months since February 2003)", 
         main = "Forecasts from AR(8) (Yule-Walker)")
```

## SARIMA(p,d,q)x(P,D,Q)
```{r}
x_sdiff = diff(x_diff,12)
tsplot(x_sdiff)
```
```{r}
acf2(x_sdiff,50)
```

# Initial Fit (based on ACF and PACF plots)
```{r}
sarima(x[,2], p=3,d=1,q=0, P=0,D=1,Q=1,S=12) 
```


# Best Fit (based on BIC)
```{r}
sarima(x[,2], p=1,d=1,q=1, P=0,D=1,Q=1,S=12) # P=1 yields AIC=14.73 and BIC=14.807
```


## Causality & Invertibility - it appears to be both 
```{r}
round(ARMAtoMA(ar=0.3345, ma=-0.7169, 25), 2)
round(ARMAtoAR(ar=0.3345, ma=-0.7169, 25), 2)
      
AR = c(1, -0.3345)
polyroot(AR)

MA = c(1, -0.7169)
polyroot(MA)
```

## SARIMA Forecast
```{r}
sarima.for(x[,2], n.ahead=24, p=1,d=1,q=1, P=0,D=1,Q=1,S=12, 
           main = "24-Month Forecast", xlab = "Time (number of months since January 2003)")
```